# ==========================================
# MM-UNet Installation Commands for FIVEs Dataset
# Designed for Python 3.10.x in cloud environments
# FOR VAST.AI PYTORCH TEMPLATE - Forces reinstall to avoid conflicts
# ==========================================
# 
# Copy and paste these commands one by one or run as a batch
# 
# ==========================================

# ==========================================
# OPTION A: Step-by-step Installation (Recommended for Vast.ai)
# ==========================================

# 1. First, ensure you have Python 3.10.x
# python3 --version  # Should show Python 3.10.x

# 2. Upgrade pip
pip install --upgrade pip

# 3. Install PyTorch 2.0.0 with CUDA 11.8 (FORCE to override Vast.ai defaults)
pip install --force-reinstall torch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 --index-url https://download.pytorch.org/whl/cu118

# 4. Install core dependencies (FORCE reinstall to ensure correct versions)
pip install --force-reinstall --no-deps timm==0.4.12
pip install --force-reinstall --no-deps objprint==0.2.3
pip install --force-reinstall --no-deps accelerate==0.18.0
pip install --force-reinstall tensorboard easydict SimpleITK monai nibabel pyyaml opencv-python openpyxl scikit-learn pandas matplotlib Pillow

# 5. Install Mamba dependencies (requires CUDA toolkit for compilation)
# Navigate to causal-conv1d and install
cd requirements/Mamba/causal-conv1d
pip install . --force-reinstall --no-build-isolation
cd ../../..

# Navigate to mamba-ssm and install
cd requirements/Mamba/mamba  
pip install . --force-reinstall --no-build-isolation
cd ../../..


# ==========================================
# OPTION B: One-liner Installation (after PyTorch) - WITH FORCE
# ==========================================

# After installing PyTorch, run this single command:
pip install --force-reinstall timm==0.4.12 objprint==0.2.3 accelerate==0.18.0 tensorboard easydict SimpleITK monai nibabel pyyaml opencv-python openpyxl scikit-learn pandas matplotlib Pillow


# ==========================================
# ALTERNATIVE: For CUDA 12.1 (if Vast.ai uses newer CUDA)
# ==========================================

pip install --force-reinstall torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121


# ==========================================
# TRAINING COMMANDS
# ==========================================

# Basic training:
python train_fives.py --data_root ./fives_preprocessed --lr 0.001 --batch_size 4 --epochs 100

# With custom parameters:
python train_fives.py \
    --data_root ./fives_preprocessed \
    --lr 0.0005 \
    --batch_size 2 \
    --epochs 200 \
    --image_size 1024 \
    --warmup 10 \
    --checkpoint_dir ./my_checkpoints \
    --gpu 0

# Resume training:
python train_fives.py \
    --data_root ./fives_preprocessed \
    --lr 0.001 \
    --batch_size 4 \
    --epochs 100 \
    --resume ./checkpoints_fives/checkpoint_epoch_0050.pth


# ==========================================
# TESTING COMMANDS
# ==========================================

# Basic testing:
python test_fives.py --checkpoint ./checkpoints_fives/best_model.pth --data_root ./fives_preprocessed

# With predictions saved:
python test_fives.py \
    --checkpoint ./checkpoints_fives/best_model.pth \
    --data_root ./fives_preprocessed \
    --save_predictions \
    --output_dir ./test_results

# With custom stats file:
python test_fives.py \
    --checkpoint ./checkpoints_fives/best_model.pth \
    --data_root ./fives_preprocessed \
    --stats_file ./checkpoints_fives/data_stats.json


# ==========================================
# TROUBLESHOOTING
# ==========================================

# If you get CUDA errors during Mamba installation:
# 1. Make sure CUDA toolkit is installed: nvcc --version
# 2. Set CUDA_HOME environment variable:
#    export CUDA_HOME=/usr/local/cuda-11.8
# 3. Try installing with: TORCH_CUDA_ARCH_LIST="7.0;7.5;8.0;8.6" pip install .

# If you get memory errors:
# 1. Reduce batch size: --batch_size 1 or --batch_size 2
# 2. Reduce image size (not recommended for FIVEs): --image_size 512

# If training loss becomes NaN:
# 1. Reduce learning rate: --lr 0.0001
# 2. The code already handles this with:
#    - Gradient clipping
#    - Independent normalization for train/test sets
#    - Smooth terms in loss functions
